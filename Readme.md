# ğŸ§  Asistente Virtual con Memoria (RAG + FastAPI)

Este backend implementa un asistente virtual inteligente con memoria de conversaciones. Utiliza RAG (Retrieval-Augmented Generation) con documentos PDF, Ollama con el modelo Mistral y FastAPI como framework principal.

---

## âœ… Requisitos tÃ©cnicos

### ğŸ“¦ `requirements.txt`
```txt
fastapi
uvicorn
sqlalchemy
psycopg2-binary
python-jose[cryptography]
passlib[bcrypt]
sentence-transformers
pymupdf
faiss-cpu
requests
email-validator
```

### âš™ï¸ Entorno virtual
```bash
python -m venv env
.\env\Scriptsctivate
pip install -r requirements.txt
```

---

## ğŸ§  IA y vectorizaciÃ³n (RAG)

### ğŸ¦™ Ollama
- Instalar desde: https://ollama.com
- Iniciar modelo:
```bash
ollama run mistral
```

### ğŸ§± FAISS Vector Store
- Cargar y trocear texto de PDFs
- Generar embeddings con `sentence-transformers`
- Guardar vector DB con FAISS (`index.faiss` y `docs.pkl`)

---

## ğŸ“ Estructura del Proyecto

```
backend-mawell/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/           â† Endpoints FastAPI
â”‚   â”œâ”€â”€ models/        â† Modelos SQLAlchemy
â”‚   â”œâ”€â”€ schemas/       â† Esquemas Pydantic
â”‚   â”œâ”€â”€ services/      â† JWT, IA, Embeddings
â”‚   â”œâ”€â”€ config.py      â† ConexiÃ³n a PostgreSQL
â”‚   â””â”€â”€ main.py        â† Entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/          â† PDFs cargados
â”‚   â””â”€â”€ vector_db/     â† FAISS + chunks serializados
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ create_index.py â† Script para vectorizar PDF
â”œâ”€â”€ media/             â† Recursos multimedia
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env (opcional)
```

---

## ğŸ“„ Modelos SQLAlchemy

### ğŸ” `User`
```python
id: int
email: str
hashed_password: str
created_at: datetime
```
Usuarios registrados con autenticaciÃ³n JWT.

### ğŸ’¬ `Conversation`
```python
id: int
user_id: int (FK â†’ User)
title: str
created_at: datetime
```
Representa un hilo de conversaciÃ³n del usuario.

### ğŸ“¨ `Message`
```python
id: int
conversation_id: int (FK â†’ Conversation)
user_id: int (FK â†’ User)
question: str
answer: str
timestamp: datetime
```
Mensaje individual enviado por un usuario y su respuesta generada.

---

## ğŸ“¦ Esquemas Pydantic

### `UserCreate`, `UserLogin`, `UserResponse`
- Entrada y salida para `/auth/register` y `/auth/login`

### `ConversationCreate`, `ConversationResponse`
- Crear conversaciones y devolver metadatos del hilo

### `ChatRequest`
```python
conversation_id: int
question: str
```

### `MessageResponse`
```python
id: int
question: str
answer: str
timestamp: datetime
```

---

## ğŸ” Servicios

### `auth_service.py`
- `hash_password()`, `verify_password()`
- `create_access_token()`, `get_current_user()`

### `embedding_service.py`
- `extract_text_from_pdf()`, `chunk_text()`, `build_vector_index()`

### `ia_service.py`
- `ask_mistral_with_context()` â€“ construye el prompt con historial y consulta a Ollama

---

## ğŸ“Œ Endpoints implementados

### ğŸ”‘ Auth
- `POST /auth/register` â†’ Registro de usuario
- `POST /auth/login` â†’ Login con JWT

### ğŸ’¬ Chat
- `POST /chat/start` â†’ Crear conversaciÃ³n
- `POST /chat/send` â†’ Enviar pregunta y guardar respuesta
- `GET /chat/{conversation_id}/messages` â†’ Ver historial

---

## ğŸ› ï¸ Comandos Ãºtiles

```bash
# Iniciar servidor FastAPI
uvicorn app.main:app --reload --port 8000

# Ejecutar script para cargar y vectorizar PDF
python -m scripts.create_index
```

---

## âœ¨ Estado actual
Sistema funcional que permite:
- AutenticaciÃ³n JWT
- Carga y recuperaciÃ³n de documentos PDF
- Consulta a IA con historial por conversaciÃ³n
- Persistencia de conversaciones y mensajes